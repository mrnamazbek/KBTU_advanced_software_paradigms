# Модельное RL с изучением динамики (MuZero Pattern)

### Коротко: что это и зачем нужно
MuZero — это «всеядный» алгоритм, который учится планировать действия, не зная правил игры. Он сам строит внутреннюю модель мира.

### Суть идеи
Традиционные алгоритмы (как AlphaZero) должны знать правила (например, как ходят фигуры в шахматах). MuZero этого не требует. Он учится предсказывать три вещи:
1. **Ценность** (насколько хороша позиция).
2. **Политику** (какой ход лучше).
3. **Награду** (сколько очков получим сразу).
Он не пытается детально воссоздать картинку мира, а учит только то, что важно для принятия решения.

### Алгоритм действий
1. **Кодирование**: Преобразование наблюдения (картинки) в скрытое состояние.
2. **Динамика**: Предсказание того, как изменится скрытое состояние после хода.
3. **Планирование**: Использование поиска по дереву (MCTS) внутри этой «вымышленной» модели.
4. **Действие**: Выполнение лучшего хода в реальном мире.

### Практический пример
MuZero достиг сверхчеловеческого уровня в шахматах, Го и всех 57 играх Atari, используя один и тот же алгоритм без знания специфики каждой игры.

### Формула (Компоненты модели)
- $s_t = h_\theta(o_1, ..., o_t)$ (Функция представления)
- $r_t, s_t = g_\theta(s_{t-1}, a_t)$ (Функция динамики)
- $p_t, v_t = f_\theta(s_t)$ (Функция прогноза)

### Советы и ошибки
- **Ошибка**: Попытка предсказать каждый пиксель следующего кадра (слишком сложно).
- **Совет**: Обучайте модель только на тех данных, которые влияют на итоговую награду и ценность.

![Визуализация](visual.png)
