# Глубокое обучение с подкреплением (Deep RL) и аппроксимация функции ценности

### Коротко: что это и зачем нужно
Это метод, который позволяет ИИ учиться принимать решения в сложной среде (например, в видеоиграх или при управлении роботом), просто наблюдая за происходящим на экране и получая «награды» за правильные действия.

### Суть идеи
В обычном обучении с подкреплением агент запоминает ценность каждого действия в каждой ситуации. Но если ситуаций миллионы (как пикселей на экране), запомнить все невозможно. Решение: использовать нейросеть как «предсказатель» (аппроксиматор), который по картинке угадывает, насколько выгодно то или иное действие.

### Алгоритм действий
1. **Наблюдение**: Агент получает текущее состояние (например, скриншот игры).
2. **Предсказание**: Нейросеть (DQN) выдает оценку (Q-значение) для каждого возможного действия (вверх, вниз, прыжок).
3. **Действие**: Агент выбирает действие с наибольшей оценкой (или пробует случайное для изучения).
4. **Обратная связь**: Агент получает награду (очки) или штраф и сохраняет этот опыт.
5. **Обучение**: Нейросеть корректирует свои веса, чтобы в будущем точнее предсказывать награду.

### Практический пример
DeepMind DQN в играх Atari. ИИ «видит» только пиксели и знает счет. Через тысячи попыток он сам понимает, что нужно отбивать мячик в Breakout, чтобы заработать очки.

### Формула (Уравнение Беллмана)
$$Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s, a)]$$
*Где $Q(s, a)$ — ценность действия $a$ в состоянии $s$, $r$ — награда, а $\gamma$ — коэффициент будущего.*

### Советы и ошибки
- **Ошибка**: Обучаться только на последних действиях.
- **Совет**: Используйте **Experience Replay** (память о прошлом опыте), чтобы агент не забывал старые уроки.

![Визуализация](visual.png)
